[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Leequarto",
    "section": "",
    "text": "About me\nHi guys, my name is Chi-Li CHIANG, I’m from Taiwan. I’ve just graduated from my previous study which is Architectural Environment Engineering in the University of Nottingham. As you can see this seemed to be a big pivot from my previous study to Urban Spatial Science. Last year I was doing something like this:  The reasons why I did that is because after three years of study, I figured out the fact that our previous investigations on building energy performances are lack of efficiency and convincing quantitative analysis. Therefore, I chose this degree to develop my skills of coding, data processing and analyzing. Since that I got a huge interesting on modeling, so I took Remotely Sensing in this Term. In my point of view, the technique of Remotely Sensing enables us to monitor the lately changes of environment and weather which helps us to develop more energy efficient and sustainable buildings."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "7  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "week1.html#section",
    "href": "week1.html#section",
    "title": "1  Week1",
    "section": "1.1 –",
    "text": "1.1 –"
  },
  {
    "objectID": "week1.html#section-1",
    "href": "week1.html#section-1",
    "title": "1  Week1",
    "section": "1.2 ",
    "text": "1.2"
  },
  {
    "objectID": "week1.html#summary",
    "href": "week1.html#summary",
    "title": "1  Week 1 An Introduction to Remote Sensing",
    "section": "1.1 Summary",
    "text": "1.1 Summary\nThis week, we have learnt about what is remote sensing and the basic theories and types of it. However, in order to keep the diary short and try to cover up more details as I can, the content of the lectures will not be fully summarized, instead, some of them will be discussed with my practical results.\n\n1.1.1 Introducing remote sensing\nRemote sensing unlike on-site observation, it involves gathering data about an object or phenomenon without coming into direct contact with it.\n\n\n\nRemote sensing\n\n\n\n\n1.1.2 Types of remote sensing\nThe types of remote sensing can be divided into passive and active. Passive sensors collect radiation emitted or reflected by the object or surrounding areas. Active sensors, on the other hand, emits energy to scan objects and areas, after which a sensor detects and measures the radiation reflected or backscattered by the target. \n\n\n1.1.3 Resolutions of remote sensing data\nThere are four categories into which remote sensing data resolutions can be separated: spatial, spectral, temporal, and radiometric. Resolution is a measurement of the amount of information that is present in a particular image. This includes wavelengths, colours, and pixels that are recorded over a given amount of time; taken together, these make up the richness of the data that is captured. An image provides us with more detailed information and improved visual clarity when it has more pixels, colours, and wavelengths. On the other hand, if these components disappear, the imagery might lose some of its clarity and detail. So, it can be said that resolution is important to the quality of remote sensing data for a range of applications.\n\n\n1.1.4 Practical results\nIn the first week, I learned about how to use SNAP to process Sentinel 2 and Landsat 8 data. In addition, Cape Town was chosen as the study area. The graph below is the screenshot of the Sentinel data: \nThen, by comparing the band 4 and band 8 of the target area, a spectural feature space can be made:  In addition, it is interesting to downscale the sentinel image from 10m to 30m and compare the two images because Landsat 8 owns wider range of spectral bands, so we use Landsat 8 to check the land covers and draw point of interests in Sentinel 8 image on SNAP. After processing the polygons we’ve drawn, several plots can be made. From the density plot below, it can be viewed the band values of each land cover. Moreover, the mean values of each band are plot in the graph with vertical lines: \n\n\n\nSpectral profiles"
  },
  {
    "objectID": "week1.html#applications",
    "href": "week1.html#applications",
    "title": "1  Week 1 An Introduction to Remote Sensing",
    "section": "1.2 Applications",
    "text": "1.2 Applications\nSENTINEL-2, which launched on June 23, 2015, was specifically designed to deliver a large amount of data and imagery. The satellite has an opto-electronic multispectral sensor for surveying with a sentinel-2 resolution of 10 to 60 m in the visible, near infrared (VNIR), and short-wave infrared (SWIR) spectral zones, including 13 spectral bands, which ensures the capture of differences in vegetation state, including temporal changes, while also minimising the impact on atmospheric photography quality.(Sentinel-2: Satellite imagery, Overview, and characteristics 2024) While Landsat 8 was launched on February 11, 2013. The Thermal Infrared Sensor (TIRS) and the Operational Land Imager (OLI) are the two scientific instruments that compose up the Landsat 8 satellite payload. The global landmass is covered seasonally by these two sensors at 30 metres (visible, NIR, and SWIR), 100 metres (thermal), and 15 metres (panchromatic) of spatial resolution. (Landsat 8 2023)\nExtending from the practical part, there are several comparisons of Landsat 8 and Sentinel-2 data. For example, Sentinel-2 has a 5-day revisit period and collects images at a spatial resolution of 10-20m, whereas Landsat 8 can only be revisited once every 16 days and has a spatial scale of 30m.(Bharathi D, R Karthi, and Geetha P, 2023) Furthermore, Sentinel-2 and Landsat 8 data were compared (Heikki Astola et al., 2019) for their ability to predict forest variables in Southern Finland’s boreal forest.They discovered that Sentinel-2 data can be used as the primary Earth observation data source for forest resource assessment."
  },
  {
    "objectID": "week1.html#reflections",
    "href": "week1.html#reflections",
    "title": "1  Week 1 An Introduction to Remote Sensing",
    "section": "1.3 Reflections",
    "text": "1.3 Reflections\nThis week I had learnt a lot about what is remote sensing is and what can we do with it. Furthermore, the in the practical part, I had compared the differences of remote sensing data from different satellites (Sentinel and Landsat) with a new software-SNAP. We’ve taken the advantages from them and done analysis. However, I have to say it is quite hard and time-consuming for me to deal with raster data on SNAP especially drawing out the point of interests part. As I mentioned, this field is completely new to me, so I got to spend more time on this module. Hopefully I will get better in couple weeks! And I am very curious how remote sensing can be related to the building energy performance, I hope I can find the answer in the next couple weeks."
  },
  {
    "objectID": "week1.html",
    "href": "week1.html",
    "title": "1  Week1",
    "section": "",
    "text": "– ## Summary ### ## Applications ## Reflections"
  },
  {
    "objectID": "week2.html",
    "href": "week2.html",
    "title": "2  Week 2",
    "section": "",
    "text": "The presentation below gives a introduction and discussion on ALOS-2 satellite."
  },
  {
    "objectID": "week4.html#summary",
    "href": "week4.html#summary",
    "title": "4  Week 4",
    "section": "4.1 Summary",
    "text": "4.1 Summary\n\n4.1.1 The London Plan 2021\nThe Greater London Authority publishes the London Plan, which is the statutory spatial development strategy for the Greater London area in the United Kingdom, and is authored by the Mayor of London. It is periodically updated. The Greater London Authority approved a new London Plan in March 2021 that includes planning for the ensuing 20 to 25 years. Moreover, in the London Plan 2021, there is a Policy G5 Urban greening section which aims to increase the greening coverage in London urban areas. Urban greening includes a variety of options such as street trees, green roofs, green walls, and rain gardens. It can help meet other policy requirements while also providing a variety of benefits such as amenity space, increased biodiversity, mitigation of the urban heat island effect, sustainable drainage, and amenity.\nThe graph below is an example of urban greening (Green wall):\n\n\n\nAn example of Green Wall\n\n\nIn order to promote greater and better urban greening, several cities have effectively implemented the “green space factor.”(UGF) The Mayor created a generic Urban Greening Factor model to help boroughs and developers determine the appropriate level of urban greening for new developments.\nThe table below shows some of the factors used to calculate the UGF:\n\n\n\nUGF factors\n\n\nFor instance, by using the factors above, the UGF can be calculated by: (Factor A x Area) + (Factor B x Area) + (Factor C x Area) etc. divided by Total Site Area. The Mayor recommends a target score of 0.4 for residential developments and 0.3 for commercial development."
  },
  {
    "objectID": "week4.html#applications",
    "href": "week4.html#applications",
    "title": "4  Week 4",
    "section": "4.2 Applications",
    "text": "4.2 Applications\nThe suggested data for supervising the greening area in London is that using the data from Copernicus which the UK involved from 1 January 2024. Due to the fact that the policy not only require the area of greening but also the water features in the city. Therefore, remote sensing might be useful for the London government to verify and monitor. As it concluded by Zehua (2024) in his paper, there are several applications of remote sensing technology. For example, monitoring vegetation with an unmanned aerial vehicle (UAV) and laser radar (LiDAR) can aid in monitoring the health and changes of vegetation. Additionally, thermal infrared remote sensing and satellite remote sensing can monitor water quality and level fluctuations. Furthermore, several studies have already been conducted to investigate the use of remote sensing technology to monitor forest areas. A study (Huimian Li et al., 2023) uses single Landsat 8 (L) remote sensing data, single Sentinel-2 (S) remote sensing data, and combined Landsat 8 and Sentinel-2 (L + S) data as data sources. In addition, four machine learning methods were applied. The study demonstrates that machine learning models based on separate Landsat 8 OLI and Sentinel-2 satellite remote sensing data can accurately predict the AGC and spatiotemporal distribution of the Shanghai urban forest. However, the research mentioned above are only focus on distinguishing forest and other land-covers from the map without classifying them with height and species. Instead, a research which done by Turner and his colleagues (DAVID P. TURNER et al., 2004) established a method with the aid of Biome-BGC carbon-cycle process model and repeated remote sensing data for several years, they successfully distinguished the land-cover type and stand age of forest in target areas."
  },
  {
    "objectID": "week4.html#reflections",
    "href": "week4.html#reflections",
    "title": "4  Week 4",
    "section": "4.3 Reflections",
    "text": "4.3 Reflections\nAs it mentioned in the previous part, the UK participates into the Copernicus plan from 2024. Therefore, the access to previous years data might be a trouble, so it is recommended to access by other ways such as Landsat and Sentinels."
  },
  {
    "objectID": "week3.html#summary",
    "href": "week3.html#summary",
    "title": "3  Week 3 Corrections",
    "section": "3.1 Summary",
    "text": "3.1 Summary\nIn week three, the methods of correction of remote sensed data are introduced. In the summary part, different correction methods are briefly introduced and the enhancement will be dicussed with the practical results.\n\n3.1.1 Correction\nThe correction of remote sensing can be divided into three different type, and this part will be mainly focused on the solutions of these corrections.\n\n3.1.1.1 Geometric correction\nDuring geometric correction, ground control points (GCPs) are collected from various sources like GPS, maps, images, and handheld devices. These points are then compared to the image and a reference dataset. Geometric transformation coefficients are calculated based on the matched GCP coordinates, defining the transformation to map coordinates from the original image to the reference dataset. These coefficients are applied to the image to minimize root mean square error (RMSE), typically aiming for an RMSE of approximately 0.5 through graph plotting and parameter adjustments. Finally, the raster image is resampled using interpolation methods such as nearest neighbor to ensure pixel alignment and accuracy across images.\n\n\n3.1.1.2 Atmospheric correction\nThe atmospheric correction methods aim to reduce atmospheric influences on photographs in order to recover surface features more accurately. They are broadly divided into relative and absolute correction approaches. Empirical Line Correction, a relative approach, uses ground spectral data and linear regression against satellite pictures to estimate surface reflectance. air Radiative Transfer Models, on the other hand, take an absolute approach, using models such as MODTRAN 4+ and 6S to calculate and rectify air interference while accounting for absorption, scattering effects, and path radiance. Another comparable method, Dark Object Subtraction (DOS), involves identifying the image’s darkest pixel values and subtracting them from all pixels to reduce ambient scattering and optical mixing.\n\n\n3.1.1.3 Orthorectification correction / topographic correction:\nOrthorectification correction is a subset of georectification that involves providing geographic coordinates to an image while removing distortions caused by terrain. Its aim is to ensure each pixel appears as if it were captured from directly overhead. To achieve this, sensor geometry information, elevation models, and orthorectification algorithms are utilized. The resulting orthorectified image accurately corresponds each pixel to a specific location on the Earth’s surface, enabling precise analysis of surface features.\n\n\n\n3.1.2 Enhancement\n\n3.1.2.1 Ratio enhancement\nThe figure below shows the ratio enhancement of vegetation with the aid of NDVI index (above 0.2): \nForm the graph, it can be viewed that the healthy vegetation in Cape Town is shown as green. And the ocean and other places without healthy vegetation is shown as white.\n\n\n3.1.2.2 Texture enhancement\nThe texture enhancement took me a lot of time running the code in R. However, I guess it is because the area I’ve chosen was too large. In the following image, the places with high homogeneity are green while in contrast, low homogeneity places are yellow or red:\n\n\n\nTexture enhancement"
  },
  {
    "objectID": "week3.html#applications",
    "href": "week3.html#applications",
    "title": "3  Week 3 Corrections",
    "section": "3.2 Applications",
    "text": "3.2 Applications\nExtending from the lecture, the applications of correction are not mentioned a lot in the content. However, when I was viewing relevant researches, the application of remote sensing correction varies a lot. For example, a study seeks to provide an effective algorithm for reducing atmospheric interference in remote sensing data by combining publicly available atmospheric correction algorithms with machine learning and artificial intelligence approaches. The objective is to reduce uncertainty in water quality monitoring and improve the accuracy of retrieving optically active water quality indicators by addressing the impact of atmospheric particles, especially aerosols, on remote sensing data.(Mir Talas Mahammad Diganta, Md Galal Uddin, and Agnieszka I. Olbert, 2023) In addition, I have noticed that the methods of correction are not limited with the three method mentioned above. When we download data sets from websites such as USGS, we tried to select data sets with 0 cloud cover, this procedure might filter out most of the data. In order to prevent this, I’ve discovered a research which done by Zhang and his colleague (Chi Zhang et al., 2023)， they developed a method for removing thin clouds from remote sensing images to improve the quality of ground surface information. The method combines an enhanced approach for locally estimating ambient light with a spectral transformation scheme to generate intermediate images and estimate transmission maps.Experiments conducted on a range of satellite photos demonstrated that the suggested approach is better than the state-of-the-art algorithms, producing images with less clouds and more accurate recovery of ground surface information. Moreover, a research had investigated the method of enhancing land surface temperature (LST) data (Qi Mao, Jian Peng, and Yanglin Wang, 2021), in which Three types of enhancement techniques were thoroughly examined: simultaneous spatiotemporal resolution enhancement, temporal enhancement, and spatial enhancement. The study also covered approaches for evaluating quality and suggested avenues for future investigation, such as combining data-driven and process-driven approaches, developing cross-referencing tools, and improving localization tactics."
  },
  {
    "objectID": "week3.html#reflections",
    "href": "week3.html#reflections",
    "title": "3  Week 3 Corrections",
    "section": "3.3 Reflections",
    "text": "3.3 Reflections\nIn week 3, I have learned a lot about correction and enhancement of remote sensed data. It is my first time to now that the remote sensing data we used got a lot limitations and potentials. Fortunately, Andy had mentioned that the remote sensing products nowadays are corrected. But, it is important to learn the principles of it.It is a bit of a pity that in the practical part, we’ve only focus on the atmospheric correction. And in the enhancement part, it is very interesting to enhance the particular texture in a remote sensing image which enable us to know What topography or composition is it made of. In addition, I’m quite curious and look forward to learning about Google Earth Engine because when I do enhancement in the practical, it took me a lot of time to run the code in R."
  },
  {
    "objectID": "week1.html#reference",
    "href": "week1.html#reference",
    "title": "1  Week 1 An Introduction to Remote Sensing",
    "section": "1.4 Reference",
    "text": "1.4 Reference\nSentinel-2: Satellite imagery, Overview, and characteristics (2024) EOS Data Analytics. Available at: https://eos.com/find-satellite/sentinel-2/ (Accessed: 04 February 2024).\nLandsat 8 (2023) NASA. Available at: https://landsat.gsfc.nasa.gov/satellites/landsat-8/ (Accessed: 05 February 2024).\nBharathi D, R Karthi, and Geetha P (2023) ‘Blending of Landsat and Sentinel images using Multi-sensor fusion’.\nHeikki Astola et al. (2019) ‘Comparison of Sentinel-2 and Landsat 8 imagery for forest variable prediction in boreal region’."
  },
  {
    "objectID": "week4.html#reference",
    "href": "week4.html#reference",
    "title": "4  Week 4",
    "section": "4.4 Reference",
    "text": "4.4 Reference"
  },
  {
    "objectID": "week3.html#reference",
    "href": "week3.html#reference",
    "title": "3  Week 3 Corrections",
    "section": "3.4 Reference",
    "text": "3.4 Reference\nMir Talas Mahammad Diganta, Md Galal Uddin, and Agnieszka I. Olbert (2023) ‘Assessing the atmospheric correction algorithms for improving the retrieval data accuracy in the remote sensing technique’.\nChi Zhang et al. (2023) ‘Thin cloud correction method for visible remote sensing images using a spectral transformation scheme’.\nQi Mao, Jian Peng, and Yanglin Wang (2021) ‘Resolution Enhancement of Remotely Sensed Land Surface Temperature: Current Status and Perspectives’."
  },
  {
    "objectID": "week5.html#summary",
    "href": "week5.html#summary",
    "title": "5  Week 5 Google Earth Engine",
    "section": "5.1 Summary",
    "text": "5.1 Summary\n\n5.1.1 What is Google Earth Engine\nGoogle Earth Engine is a useful geographic data processing engine. It makes optimal use of server-side processing and distributed computing to let users handle and analyse massive amounts of geographical data. It makes better use of computational resources by processing data using mapping functions rather than loops. Because of this functionality, Google Earth Engine is one of the most popular systems for handling massive amounts of geospatial data.\n\n\n5.1.2 What can we do with GEE\nIn Google Earth Engine (GEE), we may execute geometric operations, dataset joins, zonal statistics, data filtering, apply machine learning and TensorFlow deep learning algorithms, make online visualisations, and create scalable geospatial apps using GEE data.\n\n\n5.1.3 Practical result\nIn the practical part, we’ve chosen Delhi as the study area. We loaded the Landsat-9 data of Delhi into GEE. Furthermore, we’ve done several outputs such as PCA, NDVI and gclm which we’ve done in the previous practical. It is no doubt that the operation time in GEE is much shorter then we had with R. Moreover, it is more convenient to select the output layer with GEE.\n\n5.1.3.1 NDVI\nComparing with the NDVI which produced with R, the quality of the one from GEE is higher:\n\n\n\nNDVI output\n\n\n\n\n5.1.3.2 PCA\n\n\n\nPCA output"
  },
  {
    "objectID": "week5.html#applications",
    "href": "week5.html#applications",
    "title": "5  Week 5 Google Earth Engine",
    "section": "5.2 Applications",
    "text": "5.2 Applications\nAs it mentioned in the lecture, GEE is a powerful research tool which enables us to deal with massive geospatial data without spending a lot of time. Moreover, the applications of GEE varies a lot in different fields.For instance, the graph below categorizes the applications of GEE (Haifa Tamiminia et al., 2020):  For example, a research (M. Mohammadi and M. Akhoondzadeh, 2023) had used the Google Earth Engine platform to validate NASA’s assertion of a methane cloud near Southern Tehran. Through spatiotemporal analysis of daily data from the TROPOMI sensor, it was confirmed that landfills are the dominant source of methane emissions. As a result, the discovery proved the veracity of NASA’s report, owing mostly to the presence of a landfill in the vicinity. Therefore, the Google Earth Engine platform emerges as a valuable tool for tracking daily, monthly, and annual differences. In addition, Daeng and his colleagues (Daeng Achmad Suaidi et al., 2023) used Google Earth Engine (GEE) to analyse Landsat 8 satellite images, successfully tracking the land surface temperature of the Arjuno-Welirang volcanic complex between 2016 and 2021. The data demonstrated a considerable increase in surface temperature in the crater area in 2018, which coincided with increased seismic activity reported by the Volcanological Survey of Indonesia (VSI). This highlights GEE’s ability in accurately tracking volcanic activity with readily available satellite data."
  },
  {
    "objectID": "week5.html#reflections",
    "href": "week5.html#reflections",
    "title": "5  Week 5 Google Earth Engine",
    "section": "5.3 Reflections",
    "text": "5.3 Reflections\nGEE is really a efficient tool! It really take less time when I was doing PCA comparing to previous weeks. However, when I run the code in GEE, it always ran the whole code instead of certain trunk. In addition, as it mentioned in the application parts, the GEE got different kinds of applications but the limitations of GEE is quite severe, too. Therefore, it is quite important for us to choose appropriate tools when doing research. When the research is related to SAR data, GEE might not be applicable."
  },
  {
    "objectID": "week5.html#reference",
    "href": "week5.html#reference",
    "title": "5  Week 5 Google Earth Engine",
    "section": "5.4 Reference",
    "text": "5.4 Reference\nHaifa Tamiminia et al. (2020) ‘Google Earth Engine for geo-big data applications: A meta-analysis and systematic review’.\nM. Mohammadi and M. Akhoondzadeh (2023) ‘Monitoring and detection of methane gas in Tehran in Google Earth Engine’.\nDaeng Achmad Suaidi et al. (2023) ‘Evolution of Increased Volcanic Activity in Arjuno-Welirang Based on LST Analysis of Landsat 8 Satellite Imagery Using GEE Cloud Computing’."
  },
  {
    "objectID": "week6.html#summary",
    "href": "week6.html#summary",
    "title": "6  Week 6 Classification I",
    "section": "6.1 Summary",
    "text": "6.1 Summary\n\n6.1.1 Classification and regression trees (CART)\nCART (Classification and Regression Trees) is a decision tree approach for solving classification and regression issues. To construct the tree, the data is recursively divided into smaller subsets and simple models are built on each. Each node in the tree represents a characteristic, each branch represents a potential value for that feature, and the leaf nodes include either categories (for classification problems) or numerical values (for regression problems).\n\n6.1.1.1 Overfitting\nOverfitting occurs when a model highly adapts to the features and noise in training data, resulting in poor performance on new data. Methods for addressing overfitting include restraining decision tree growth, pruning, and modifying regularisation parameters. Limiting the growth of decision trees lowers model complexity by imposing constraints such as minimum sample size in leaf nodes, whereas pruning decreases complexity by deleting specific nodes from the tree. Adjusting regularisation parameters, such as raising them, can help to reduce model complexity and the danger of overfitting. When evaluating model performance, data is often separated into training and testing sets, and model performance is compared.\n\n\n6.1.1.2 Random forest (RF)\nRandom Forest is an technique that builds several decision trees and combines them for classification or regression problems. During each tree’s training, it reduces overfitting by using random sampling and random feature selection, while evaluating the model’s performance with out-of-bag data. Its main advantages include its capacity to handle huge datasets and high-dimensional data, as well as its resilience to noise. Random Forest uses out-of-bag (OOB) error estimation to provide an accurate assessment of model performance without requiring extra validation datasets.\n\n\n\n6.1.2 Practical result\nIn the practical part, we’ve use two train test (Random forest). However, the different between them is that the first one is done by ploygons while the other one is done by using pixels. By comparing the result, the one used polygons is with plenty of errors and the accuracy of the model is unacceptable.\n\n\n\nRF polygon\n\n\nIn contrast, the result by using pixels is more acceptable and reasonable. But the results shows that the accuracy of the model is about 100% which is not as expected. It can be known that overfitting might be occur during RF pixel. In addition, the possible reason that occurring overfitting is that when I get ploygon for different land covers, I try to avoid 5000 elements which is the highest capability for GEE so the polygon might be too small as training data for RF. The pixels from the polygon may not be representative. Moreover, unlike SNAP, when I draw polygons, we got Spectrum Viewer to check which land cover I am choosing but we don’t have it in GEE so there might cause some error for training data.\n\n\n\nRF pixel"
  },
  {
    "objectID": "week6.html#applications",
    "href": "week6.html#applications",
    "title": "6  Week 6 Classification I",
    "section": "6.2 Applications",
    "text": "6.2 Applications\nAs it mentioned in the practical result, overfitting might occur when using CART. In order to solve the weakness of CART. Some alternative methods had been developed. For example, Boosted CART is a method for improving decision tree predictions that may be applied to both classification and regression tasks. It generates trees successively, each using information from the previous one and fitting them to modified versions of the original dataset. As a result, Boosted CART excels at handling large datasets and complex trees. Its value is in integrating numerous decision trees to produce forecasts with smaller variance, hence overcoming CART’s constraints.Furthermore, Samir and his colleagues (Samir Barman, Ramasubramanian V, and Mrinmoy Ray, 2019) compared the performances of CART and Boosted CART with the same data from agricultural ergonomics. They found out that in terms of classification accuracy, Boosted CART outperforms CART on the same datasets. Moreover, when I was viewing the lecture slides, a graph caught me:\n The graph shows the comparisons of different machine learning classification algorithms. From the graph, it can be inferred that when comparing with RF, CART has lower accuracy, but it owns higher interpretability. When I was confusing what makes the difference between accuracy and interpretability, a research solved my question. In the research (Anish Kumar, Sanjeev Sinha, and Samir Saurav, 2023), they’ve investigated the possibilities of stabilising clayey soil with cement and fly ash to strengthen the subgrade pavement layer with RF, multiple regression and CART. The following table shows the result of R square:\n It can be noticed that CART owns the highest R squared result among three algorithms. It is might because that the topic of this research is try to explain the relationship between unconfined compressive strength (UCS) and nonlinear relationships with curing time and binder content.In contrast of classification, which is better with RF because RF may provide higher accuracy."
  },
  {
    "objectID": "week6.html#reflections",
    "href": "week6.html#reflections",
    "title": "6  Week 6 Classification I",
    "section": "6.3 Reflections",
    "text": "6.3 Reflections\nOnce again, GEE is a very powerful tool. I couldn’t imagine how time consuming we do remote sensed data classification in R. Moreover, CART and RF are very interesting because it can be used in many different fields, we also learnt about them in Data science module but in Data science we focused more on regression but classification in this module. For me, as it mentioned in application, it is important to choose the right machine learning algorithm because all of them can be better at accuracy and interpretability.However, further learning should be applied because it occurred overfitting when I was doing the practical though I’ve redraw my polygons for severral times, the accuracy results show 100%."
  },
  {
    "objectID": "week6.html#reference",
    "href": "week6.html#reference",
    "title": "6  Week 6 Classification I",
    "section": "6.4 Reference",
    "text": "6.4 Reference\nSamir Barman, Ramasubramanian V, and Mrinmoy Ray (2019) ‘An application of Boosted Classification and Regression Trees (CART) in agricultural ergonomics’.\nAnish Kumar, Sanjeev Sinha, and Samir Saurav (2023) ‘Random forest, CART, and MLR‑based predictive model for unconfined compressive strength of cement reinforced clayey soil: a comparative analysis’."
  },
  {
    "objectID": "week7.html#summary",
    "href": "week7.html#summary",
    "title": "7  Week 7 Classification II",
    "section": "7.1 Summary",
    "text": "7.1 Summary\n\n7.1.1 Object based image analysis (OBIA)\nObject-based image analysis (OBIA) is an advanced analytical method for remote sensing images that identifies ground features by segmenting images into several objects and taking into account their spatial and spectral properties. Common superpixel approaches, such as the SLIC algorithm, may generate representative objects, while software packages like Supercells allow for variable parameter adjustments. OBIA also includes feature extraction and classification steps, which combine average spectral values with other morphological data to classify objects. Furthermore, more advanced technologies, such as SegOptim, can incorporate other algorithms, expanding segmentation and classification possibilities to improve OBIA capability. In summary, OBIA effectively detects land features using object-level analysis and processing, providing critical support for geographical information systems and other domains.\n\n\n7.1.2 Sub pixel analysis\nWhen performing subpixel analysis in urban environments, it is critical to consider factors such as pixel mixing, suitable endmember selection, and computing complexity. The V-I-S model simplifies land cover classification, whereas multiple endmember spectral analysis (MESMA) calculates the fraction of each land cover type inside pixels. These methods, which take into account computational resources and spectrum libraries, improve the accuracy and usability of subpixel analysis, allowing for better urban land cover monitoring and mapping. Subpixel analysis is critical in metropolitan environments due to different land cover types and complex spatial distributions, which frequently result in mixed land cover within pixels. It delivers exact land cover data for urban planning, environmental monitoring, and resource management.\n\n\n7.1.3 Accuracy assessment\nAccuracy assessment is a critical job used to evaluate the performance of classification models. In this phase, we concentrate on Producer Accuracy (PA) and User Accuracy (UA), which reflect the consistency of classification results with ground truth data, as well as the usability of classification findings for users. The Kappa coefficient, a commonly used statistic, seeks to assess the consistency of classification results with random results, taking into account random agreement and offering a standardised value. While the Kappa coefficient has positives, it is also criticised for its great complexity and susceptibility to class imbalances. To summarise, the suitable metrics are determined by the unique application and data properties.\n\n\n7.1.4 Practical result\nThe practical for this week is also focused on the classification but with the method of OBIA and Sub-pixel.However, I found out that the results of these two methods varies a lot.The screenshot below is the result of sub-pixel:  It can be viewed that the forest is scattered in the whole area (Green) while the bare earth (Grey) and Urban (Pink) owns the most proportion.However, the result of OBIA shows that the forest and grass are the majority land covers in the study area.  The reason causes this huge difference can be concluded that: The data might not be taken properly. To be instance, I took 7 polygons for each land cover. However, as I mentioned in last week, it is hard for me to identify what land cover it is when I was drawing especially for tree and grass. And the OBIA analysis the areas with shapes and identify the landcover types by the properties of each shape, so the areas with similar properties might be identified as the same. In this case, the area of grass and forest might be identified as the same. Also, from the original image of study area (Daressalaam), it can be viewed that the bare earth and urban are surrounded with grass and forest, so it may cause errors when doing the classification. However, in my point of view, by comparing the results with the actual land cover, I think that sub-pixel method might be more applicable to this case. The specific accuracy analysis should be used in order to give out the result as we’ve done last week."
  },
  {
    "objectID": "week7.html#applications",
    "href": "week7.html#applications",
    "title": "7  Week 7 Classification II",
    "section": "7.2 Applications",
    "text": "7.2 Applications\nIn this week, we’ve introduced the classification methods of OBIA and sub-pixel. However, it troubles me that how to determine whether OBIA and sub-pixel analysis to use especially when I had done the practical which the results of the two methods varied a lot. After searching relevant literature and research, I found a research which done by Sumbal, Shah and their colleagues(Sumbal Bahar Saba et al., 2022), had investigated investigate and assessed the advantages and disadvantages of pixel, sub-pixel, and Object-Based Image Analysis (OBIA)-based approaches for identifying co-seismic landslides in Muzaffarabad, Pakistan. They found out that the accuracy of sub-pixel method is 90.9% while the one of OBIA is 91.4%.  For instance, they’ve indicated that when using OBIA, high resolution images might be required while sub-pixel is a more cost-efficient way. Therefore, I inferred that OBIA and sub-pixel own their pros and cons but OBIA can get high accuracy with high resolution images while the one of sub-pixel can perform well in medium resolution images. Moreover, a research (Y. Gao and J.F. Mas, no date)had compared the performance of OBIA and sub-pixel in four different resolutions, they concluded that OBIA has advantages on dealing high resolution images and the advantages become more and more significant with the increase of resolutions."
  },
  {
    "objectID": "week7.html#reflections",
    "href": "week7.html#reflections",
    "title": "7  Week 7 Classification II",
    "section": "7.3 Reflections",
    "text": "7.3 Reflections\nThe lecture for this week indeed took me some time to get understand especially for the accuracy part because I think it is the most important part due to the fact that if we cannot figure out the most suitable method for doing the classification, the most convincing way is to use accuracy test to evaluate different methods. However, as I have mentioned several time in this week and a couple weeks before, it been a challenge for me to identify the land covers when drawing polygons, I know that it works better when I zoom in but then the polygons might be too small then the accuracy would be affected when we use pixel based methods. Although I encounter many challenges using GEE but I really enjoy it cause it is a really sufficient application."
  },
  {
    "objectID": "week7.html#reference",
    "href": "week7.html#reference",
    "title": "7  Week 7 Classification II",
    "section": "7.4 Reference",
    "text": "7.4 Reference\nSumbal Bahar Saba et al. (2022) ‘Comparison of pixel, sub‑pixel and object‑based image analysis techniques for co‑seismic landslides detection in seismically active area in Lesser Himalaya, Pakistan’.\nY. Gao and J.F. Mas (no date) ‘A COMPARISON OF THE PERFORMANCE OF PIXEL-BASED AND OBJECT-BASED CLASSIFICATIONS OVER IMAGES WITH VARIOUS SPATIAL RESOLUTIONS’."
  },
  {
    "objectID": "week8.html#summary",
    "href": "week8.html#summary",
    "title": "8  Week 8 Synthetic Aperture Radar (SAR) data",
    "section": "8.1 Summary",
    "text": "8.1 Summary\n\n8.1.1 SAR data\nThere are several types of Synthetic Aperture Radar (SAR) data, including power scale, amplitude scale, and dB scale, each of them provides distinct insights on surface properties.\nFor instance, when choosing SAR data, it is important to consider the target detection goal, such as recognizing surface roughness or material volume and geographical factors. Different types of SAR data can be suitable for different ambitions, for example, amplitude scale is suitable for visualization and dB scale is for dark pixel differences. Therefore, choosing the appropriate SAR data ensure effective analysis according to objectives and environmental circumstances.\n\n\n8.1.2 Identify changes with SAR data\nThere are four common methods for identifying changes in SAR images: mean ratio images, log ratio images, improved ratio log ratio images, and original ratio images. Mean ratio images show changes by calculating the ratio of mean values from corresponding neighborhoods in two photos while log ratio images make changes more visible by taking the natural logarithm of the pixel value ratio between the two images. In addition, improved ratio log ratio images use improved ratio techniques and logarithmic transformation to increase sensitivity to changes and reduce dynamic range. Then, the original ratio images entail directly computing the pixel value ratio between two photos.\nMoreover, with the high temporal nature of SAR data, we can also identify the changes through t-tests and standard deviations. However, SAR data often do not follow a normal distribution, but rather gamma or other skewed distributions, making typical statistical tests difficult to apply. To solve this, data transformation or non-parametric testing techniques might be used. Change detection in SAR data is often performed using continuous change detection algorithms that do not rely on specific data distribution assumptions. As a result, while working with SAR data, it is critical to use proper statistical methods and carefully analyse the data features to ensure reliable analysis results.\n\n\n8.1.3 Practical result\nThe practical this week is really interesting, we investigated how the Beirut was changed before and after the blast on August 4th 2020 with Sentinel-1 images. Furthermore, the analysis is done by using t-test and standard deviation. To be more specific, the standard deviation was used to measure the degree of variation in pixel values, the higher the standard deviation, the wider the fluctuation range of pixel values. In addition, the T test was then performed to see if there were any significant differences in the mean value of image pixel values before and after the blast, and the ratio of mean difference to standard deviation was determined to assess whether the changes were statistically significant.\nThe screenshot below shows the result of the changes. Blue indicates no damage while red means high damages.  Surprisingly, the result of our analysis shows that number of damaged buildings we estimated only have 8% of difference with the one estimated by the U.N. Although the result is very promising and interesting, the author also had mentioned the limitations such as that some of the damages of buildings can not be captured from the view of satellites. Moreover, I think that the result should be further validated because it cannot be ensured that the damaged buildings we estimated were actually damaged, instead, the change we found might related to other factors despite the fact that we’ve take mean value but the potential errors cannot be neglected. However, the best way to validate is to have an actual site visit to compare the estimate result and real situation. But it is time and money consuming. As the author concluded, the blast is a three-dimensional problem, but the images we used are two-dimensional."
  },
  {
    "objectID": "week8.html#applications",
    "href": "week8.html#applications",
    "title": "8  Week 8 Synthetic Aperture Radar (SAR) data",
    "section": "8.2 Applications",
    "text": "8.2 Applications\nThe practical this week really enables me to be curious about the application we can do with SAR data. After digging in, I found some innovative research. A research which done by Donato and his colleague (Donato Amitrano et al., 2024) investigated flood detection approaches employing SAR imagery. The analysis focuses on main flood detection technologies, performance evaluation criteria, and publicly available SAR datasets. Discussing the applications and limitations of SAR-based flood detection in various situations. Specifically, in vegetated and urban regions, where complicated scattering mechanisms may influence the precision of water extraction:\n\n\n\nSchematic representation of scattering phenomena\n\n\nTo deal with these problems, recommendations include developing tools for fair comparison and extensive technique validation, as well as investigating complementary remote sensing technologies such as Global Navigation Satellite System-Reflectometry (GNSS-R). Both the research and the practical had mentioned about the accuracy of the identification of changes, in the practical we’ve mentioned that the influences of blast we estimated may have include that caused by other factors, while the research had indicated that when SAR used in specific scenarios, the reflection might be affected by vegetation. Therefore, it is important to identify the changes with proper images. In addition, Wenjie and Yanping had (Wenjie Shen et al., no date) carried out a method called SAR-shift in which provides a new change detection algorithm for spaceborne SAR time-series data using SAR-SIFT-Logarithm Background Subtraction. It first pre-processes the input time series data, which includes noise reduction and radiometric calibration. It then uses SAR-SIFT coregistration to protect detection performance from mismatches in order to extract the static background, areas that don’t change over time are found using a median filter. In comparison to existing pairwise comparison approaches, the suggested method detects overall change information more effectively and takes less time to process. In comparison, what we had done in the practical, we used standard deviation and t-test to evaluate the changes. Further development should be done to know which one is better."
  },
  {
    "objectID": "week8.html#reflections",
    "href": "week8.html#reflections",
    "title": "8  Week 8 Synthetic Aperture Radar (SAR) data",
    "section": "8.3 Reflections",
    "text": "8.3 Reflections"
  },
  {
    "objectID": "week8.html#reference",
    "href": "week8.html#reference",
    "title": "8  Week 8 Synthetic Aperture Radar (SAR) data",
    "section": "8.4 Reference",
    "text": "8.4 Reference\nDonato Amitrano et al. (2024) ‘Flood Detection with SAR: A Review of Techniques and Datasets’.\nWenjie Shen et al. (2023) ‘Spaceborne SAR Time-Series Images Change Detection Based on SAR-SIFT-Logarithm Background Subtraction’."
  }
]