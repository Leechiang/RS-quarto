# Week 6 Classification I

## Summary

### Classification and regression trees (CART)

CART (Classification and Regression Trees) is a decision tree approach for solving classification and regression issues. To construct the tree, the data is recursively divided into smaller subsets and simple models are built on each. Each node in the tree represents a characteristic, each branch represents a potential value for that feature, and the leaf nodes include either categories (for classification problems) or numerical values (for regression problems).

#### Overfitting

Overfitting occurs when a model highly adapts to the features and noise in training data, resulting in poor performance on new data. Methods for addressing overfitting include restraining decision tree growth, pruning, and modifying regularisation parameters. Limiting the growth of decision trees lowers model complexity by imposing constraints such as minimum sample size in leaf nodes, whereas pruning decreases complexity by deleting specific nodes from the tree. Adjusting regularisation parameters, such as raising them, can help to reduce model complexity and the danger of overfitting. When evaluating model performance, data is often separated into training and testing sets, and model performance is compared.

#### Random forest (RF)

### Practical result

In the practical part, we've use two train test (Random forest). However, the different between them is that the first one is done by ploygons while the other one is done by using pixels. By comparing the result, the one used polygons is with plenty of errors and the accuracy of the model is unacceptable.

![RF polygon](img/屏幕截图%202024-03-13%20191356.jpg)

In contrast, the result by using pixels is more acceptable and reasonable. But the results shows that the accuracy of the model is about 100% which is not as expected. It can be known that overfitting might be occur during RF pixel.

![RF pixel](img/屏幕截图%202024-03-13%20175926.jpg)

## Applications

## Reflections

## Reference
