# Week 6 Classification I

## Summary

### Classification and regression trees (CART)

CART (Classification and Regression Trees) is a decision tree approach for solving classification and regression issues. To construct the tree, the data is recursively divided into smaller subsets and simple models are built on each. Each node in the tree represents a characteristic, each branch represents a potential value for that feature, and the leaf nodes include either categories (for classification problems) or numerical values (for regression problems).

#### Overfitting

Overfitting occurs when a model highly adapts to the features and noise in training data, resulting in poor performance on new data. Methods for addressing overfitting include restraining decision tree growth, pruning, and modifying regularisation parameters. Limiting the growth of decision trees lowers model complexity by imposing constraints such as minimum sample size in leaf nodes, whereas pruning decreases complexity by deleting specific nodes from the tree. Adjusting regularisation parameters, such as raising them, can help to reduce model complexity and the danger of overfitting. When evaluating model performance, data is often separated into training and testing sets, and model performance is compared.

#### Random forest (RF)

### Practical result

In the practical part, we've use two train test (Random forest). However, the different between them is that the first one is done by ploygons while the other one is done by using pixels. By comparing the result, the one used polygons is with plenty of errors and the accuracy of the model is unacceptable.

![RF polygon](img/屏幕截图%202024-03-13%20191356.jpg)

In contrast, the result by using pixels is more acceptable and reasonable. But the results shows that the accuracy of the model is about 100% which is not as expected. It can be known that overfitting might be occur during RF pixel.

![RF pixel](img/屏幕截图%202024-03-13%20175926.jpg)

## Applications
As it mentioned in the practical result, overfitting might occur when using CART. In order to solve the weakness of CART. Some alternative methods had been developed. For example, Boosted CART is a method for improving decision tree predictions that may be applied to both classification and regression tasks. It generates trees successively, each using information from the previous one and fitting them to modified versions of the original dataset. As a result, Boosted CART excels at handling large datasets and complex trees. Its value is in integrating numerous decision trees to produce forecasts with smaller variance, hence overcoming CART's constraints.Furthermore, Samir and his colleagues (Samir Barman, Ramasubramanian V, and Mrinmoy Ray, 2019) compared the performances of CART and Boosted CART with the same data from agricultural ergonomics. They found out that in terms of classification accuracy, Boosted CART outperforms CART on the same datasets.

## Reflections

## Reference
Samir Barman, Ramasubramanian V, and Mrinmoy Ray (2019) ‘An application of Boosted Classification and Regression Trees  (CART) in agricultural ergonomics’.
