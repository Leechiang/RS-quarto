# Week 6 Classification I

## Summary

### Classification and regression trees (CART)
CART (Classification and Regression Trees) is a decision tree approach for solving classification and regression issues. To construct the tree, the data is recursively divided into smaller subsets and simple models are built on each. Each node in the tree represents a characteristic, each branch represents a potential value for that feature, and the leaf nodes include either categories (for classification problems) or numerical values (for regression problems).

#### Classification tree
Classification trees partition the dataset recursively using predictor variables to maximise the purity of the resulting subsets, utilising impurity measurements such as Gini impurity. Unlike regression trees, they use impurity measures to identify optimal splits. Gini impurity evaluates the likelihood of misclassifying a randomly chosen sample if labelled according to the class distribution in a subset. The splitting criterion chooses the predictor variable and split point with the lowest Gini impurity in the resulting subsets. This recursive operation continues until the halting criteria are met, yielding a tree structure with each leaf node representing a class label.

#### Regression tree

### Practical result
In the practical part, we've use two train test (Random forest). However, the different between them is that the first one is done by ploygons while the other one is done by using pixels. By comparing the result, the one used polygons is with plenty of errors and the accuracy of the model is unacceptable. 
![RF polygon](img/屏幕截图 2024-03-13 191356.jpg)
In contrast, the result by using pixels is more acceptable and reasonable. But the results shows that the accuracy of the model is about 100% which is not as expected. It can be known that overfitting might be occur during RF pixel.

![RF pixel](img/屏幕截图 2024-03-13 175926.jpg)

## Applications

## Reflections


## Reference




